---
title: "Statistical Modelling and Inference - Seminar 2"
author: "Paul Rognon and David Rossell"
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---


```{r}
knitr::opts_chunk$set(message= FALSE, warning = FALSE)
```

```{r}
library(bayestestR)
library(rstanarm)
library(ggplot2)
library(tidyverse)
library(mombf)
PATH= 'C:/Users/paulr/Documents/PhD/teaching/StatInference/seminar2'
```

# Bayesian inference with `rstanarm`

Stan is a programming paradigm for Bayesian statistical modeling and high-performance computation written in C++ and RStan is the R interface to Stan. Within RStan, `rstanarm` is a package that emulates functions of popular frequentist R packages but with Bayesian inference (e.g. lm, glm, lmer) with the objective to make Bayesian inference more accessible. 
The graduate school applications data features, for n = 400 individuals, the admission binary variable (yes=1/no=0) and 3 covariates: score in the GRE exam, Grade Point Average of their academic transcript, and the prestige of the institution where they obtained the undergraduate degree ranked from 1 (high prestige) to 4 (low prestige).

```{r}
mydata= read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
mydata$gre= as.numeric(scale(mydata$gre))
mydata$gpa= as.numeric(scale(mydata$gpa))
mydata$rank= factor(mydata$rank)
```

We want to fit a regression model of admission against the 3 covariates. To fit a Bayesian regression with can use `rstanarm` function `stan_glm`. The response variable is binomial so we set the function parameter `family` to `binomial()`. The model can be fit via (Hamiltonian) MCMC by setting `algorithm` to `"sampling"` or via a Normal approximation to the posterior by setting `algorithm` to `"optimization"`. Mean-field variational approximation is also possible by setting `algorithm` to `"meanfield"`.

```{r}
fit <- stan_glm(admit ~ gre + gpa + rank, data=mydata, family=binomial(), algorithm='sampling', refresh=0) #MCMC
fit2 <- stan_glm(admit ~ gre + gpa + rank, data=mydata, family=binomial(), algorithm='optimizing', refresh=0) #Normal approx at posterior mode
```

We can easily obtain point estimates and confidence intervals with the functions `coef` and `posterior_interval`. We compare below the parameter estimates from these 2 computational methods, for these data they're all very similar.

```{r}
m= coef(fit)
q= posterior_interval(fit)[1:length(m),]
b.mcmc= round(cbind(m, q), 3)
cat('via MCMC\n')
b.mcmc

m2= coef(fit2)
q2= posterior_interval(fit2)[1:length(m2),]
b.lapl= round(cbind(m2, q2), 3)
cat('\nvia Normal approximation of posterior \n')
b.lapl
```
## Priors in `rstanarm`

Another important feature of `rstanarm` that makes our life easy is that it provides default priors for models parameters. By default, `rstanarm` sets weakly informative priors. The function `prior_summary` outputs details on the priors used. Default priors can be changed using `stan_glm` function's parameter `prior` and `prior_intercept`. See [`rstanarm` documentation](https://mc-stan.org/rstanarm/articles/priors.html) for more details. 

```{r}
prior_summary(fit)
```
## MCMC algorithm in `rstanarm`

`rstanarm` makes our life easy but a lot is happening in the back. `rstanarm` provides default values for many parameters of the Bayesian estimation. For example, by default, the MCMC algorithm uses 4 chains with 2,500 iterations each. We plot below the trace of chains for the intercept and the GRE coefficient.

```{r}
plot(fit, "trace", pars = c("(Intercept)"),
     facet_args = list(nrow = 2)) + 
  ggplot2::scale_color_discrete() + 
  ggplot2::ggtitle('Intercept')
plot(fit, "trace", pars = c("gre"),
     facet_args = list(nrow = 2)) + 
  ggplot2::scale_color_discrete()  + 
  ggplot2::ggtitle('GRE')
```

What do you observe in the plot above? What do you infer with respect to the convergence of the chains in the MCMC algorithm?

```{css}
#write your answer here



```

With the function `describe_posterior` from the package `bayestestR` we can obtain certain MCMC convergence diagnostics such as the effective sample size (ESS) and Gelman-Rubin statistic (Rhat).

The ESS corresponds to the number of independent samples with the same estimation power as the drawn autocorrelated samples. It is a measure of “how much independent information there is in autocorrelated chains”.

The Gelman-Rubin statistic compares the variation between chains to the variation within the chains.

```{r}
describe_posterior(fit)
```

What is your interpretation of the diagnostic values above?

```{css}
#write your answer here



```



# Bayesian Gaussian regression: prior elicitation and comparision to MLE 

## Prior elicitation

When priors are not pre-defined, we have to pick a prior distribution for each parameter and set its hyperparameters. In Bayesian Gaussian regression with Normal shrinkage prior for $\beta|\rho$ an important hyperparameter to set is $g$:

$$p(\beta|\rho) = N(\mathrm{0},g\rho I)$$
One strategy is to set $g$ via prior elicitation, i.e. ensuring that it encodes prior beliefs
that are minimally sensible. In this case, we will inspect how the prior proportion of variance explained by the linear regression $\tau$ (theoretical $R^2$ coefficient) changes with $g$. Indeed, choosing $g$ implies encoding one's prior beliefs about $\tau$ as we will see more clearly below.

We will conduct prior elicitation on $g$ for the salary dataset from Seminar 1. We first load the data.

```{r}
load(file.path(PATH, 'data/salary.RData'))
```

We then create the design matrix.

```{r}
X= model.matrix(~ female + marital + edu + region + exp1 +
 female:marital + female:edu + female:region + female:exp1
 + marital:edu + marital:region + marital:exp1 + edu:region + edu:exp1 + region:exp1, data=salary)
y= salary$lnw
n= nrow(X)
```

For a fixed grid of values for $g$, we will obtain prior expectations of $\tau$ defined as: 
$$\tau = \left(1+\frac{n \rho}{\beta^{T} X^{T} X \beta}\right)^{-1}$$
We set our grid of values for $g$:

```{r}
gseq= seq(.1,10,length=10)
gseq
```

Now, if $\beta \sim N(\mathrm{0},g\rho I)$, then $\beta$ 
and $\sqrt{g\rho}\tilde{\beta}$ where $\tilde{\beta}\sim N(\mathrm{0},I)$ have the same distribution. Therefore, we rewrite $\tau$ as:
$$\tau = \left(1+\frac{n \rho}{\sqrt{g\rho}\tilde{\beta}^{T} X^{T} X\sqrt{g\rho}\tilde{\beta}}\right)^{-1} = \left(1+\frac{1}{g}\frac{n}{\tilde{\beta}^{T} X^{T} X\tilde{\beta}}\right)^{-1} = \left(g\frac{\tilde{\beta}^{T} X^{T} X\tilde{\beta}}{n} \right) \bigg/ \left(g\frac{\tilde{\beta}^{T} X^{T} X\tilde{\beta}}{n}+1\right) $$

Then to obtain a prior expectation of $\tau$ for each value of $g$, we first get 1,000 prior simulations for $\tilde{\beta}$. For each simulation, we obtain $\tilde{\beta}^T X^T X\tilde{\beta}/n$.

```{r}
library(mvtnorm)
V= diag(ncol(X))
beta= rmvnorm(1000, sigma= V)
sse= rowSums((X %*% t(beta))^2) / n
```

For each value of $g$, we obtain the prior mean of $\tau$ over the prior simulations:

```{r}
gseq= seq(.1,10,length=10)
r2= double(length(gseq))
for (i in 1:length(gseq)) {
  r2[i]= mean(sse * gseq[i] / (1 + sse * gseq[i]))
}
```

We plot the results:
```{r}
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(gseq, r2, type='l', xlab='g', ylab=expression(paste('Theoretical ',R^2)))
```

What value of $g$ would you pick?
```{css}
#write your answer here



```

## Comparison to MLE estimates

We fit our model for the salary data via ordinary least squares.

```{r}
fit.mle= lm(y ~ X[,-1]) #1st column in x is the intercept, already added by lm
b.mle= coef(fit.mle)
```

We fit the same model via Bayesian Gaussian regression and MCMC using default parameters of `stan_glm` and plot the two estimates.

```{r}
fit.bayes <- stan_glm(y ~ X[,-1], family = gaussian(link = "identity"), algorithm='sampling', refresh=0)
b.bayes= coef(fit.bayes)
```
```{r}
data.frame(mle= b.mle[-1], bayes= b.bayes[-1]) %>% 
  ggplot(aes(x=mle,y=bayes)) + 
  geom_point(shape = "O",size=2) +
  geom_abline(slope=1, intercept = 0, linetype = 'dashed') +
  geom_hline(yintercept = 0, linetype = 'dotted') +
  xlab('MLE OLS') +
  ylab('MCMC Bayesian regression') +
  coord_cartesian(xlim=c(-2,0.5),ylim=c(-2,0.5)) +
  theme_classic()
```

What do you observe? Is it surprising?
```{css}
#write your answer here



```


# Bayesian model selection

We now conduct Bayesian model selection and averaging on the simulated data from Seminar 1. the function `simdata.cs` generates, for each of `n` independent individuals, `p` covariates that are normally-distributed with mean 0, variance 1, and all pairwise correlations equal to `rho`.

We set `n=100`, `p=200`, `rho=0.5`, that is more features than observations. The true value of all regression coefficients is 0, except for the last four. You're encouraged to play around and explore how the results vary as one has different `n`, `p`, `rho` or regression coefficients. 

```{r}
simdata.cs <- function(seed,n,theta,phi=1,rho) {
#Simulate n observations from a linear model y= X %*% theta + e, where e ~ N(0,phi) and the rows in X ~ Normal(0,Sigma) and Sigma is compound symmetric (Sigma[i,i]=1, Sigma[i,j]=rho)
#Input
# - seed: random number seed
# - n: sample size
# - theta: true regression coefficients
# - phi: residual variance
# - rho: true pairwise correlation between variables
#Output: list with the following elements
# - y: response
# - x: n * length(theta) matrix with predictors
    require(mvtnorm)
    S <- diag(length(theta))
    S[upper.tri(S)] <- S[lower.tri(S)] <- rho
    set.seed(seed)
    x <- rmvnorm(n=n,sigma=S)
    y <- x %*% theta + rnorm(n,sd=sqrt(phi))
    return(list(y=y,x=x))
}

n= 100  # No. observations
p= 200  # No. features
rho= 0.5  # True pairwise correlation across features
beta= c(rep(0, p - 4), -2/3, 1/3, 2/3, 1)

# Simulate data
sim.data= simdata.cs(seed = 2, n, beta, phi = 1, rho)
X= sim.data[['x']]  # covariates
y= sim.data[['y']]  # response
data = data.frame(cbind(y,X))
colnames(data)[1] <- 'y'
```

## Estimation

We run the Bayesian model selection implemented in the function `modelSelection` from R package `mombf`. The option `enumerate`, if set to `TRUE`, forces full model enumeration, but more generally when the number of parameters is large it’s better to use the default, which uses Gibbs sampling to search the model space.

We set a Beta-Binomial(1,1) prior on the models and Zellner’s prior on the regression coefficients. The prior dispersion parameter `taustd` corresponds to $g$ in our notation, for which we use the default $g=1$.

```{r}
fit.bayesreg <- modelSelection(y=y,x=X, priorCoef=zellnerprior(taustd=1), priorDelta=modelbbprior(1,1))
```


## Inference

The function `postProb` outputs models posterior probilities $p(\gamma|\mathbf{y})$ where the column `modelid` lists the variables included in each model. We show the top 10 models.

```{r}
head(postProb(fit.bayesreg),10)
```
With the method `coef` we extract Bayesian model averaging estimates for each coefficient, posterior intervals and posterior marginal inclusion probabilities ($P(\beta_j \neq 0 | \mathbf{y}) = P(\gamma_j = 1 | \mathbf{y})$). 

```{r}
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3]= round(ci.bayesreg[,1:3], 3)  
ci.bayesreg[,4]= round(ci.bayesreg[,4], 4)      
head(ci.bayesreg)
tail(ci.bayesreg)
```
Just like we did in Seminar 1, we tabulate the number of true positives, false positives and false negatives in the winning model from BMS.

```{r}
sel.bayesreg = rep(FALSE, nrow(ci.bayesreg))
sel.bayesreg[c(197,198,199)] <- TRUE
table(sel.bayesreg, beta != 0)
```

We plot the CIs as we did in Seminar 1.

```{r}
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI', xlab='', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:p, beta, pch = 16)
```

How do you interpret the results above in terms of model selection? How does BMS perform compared to the LASSO inference methods from Seminar 1?
```{css}
#write your answer here



```

## Connection between BMS and $L_0$ criteria

If we specify the right prior for regression coefficients and the Beta-Binomial(1,1) on the model space, it can be shown that finding the best model by BMS is essentially equivalent to maximizing the EBIC.

$$EBIC \approx -2 \left( p(\gamma|\mathbf{y})+\log{(p+1)}\right)$$
In a purely frequentist setting, if we wanted to find the model achieving the best value of the EBIC, this would require enumerating $2^p$ variable configurations. When $p$ is large this becomes computationally unfeasible.

Instead, we can we use BMS implemented in function `modelSelection` of `mombf` with `priorCoef=bicprior()` and `priorDelta=modelbbprior()`. 

We now apply this selection procedure to the spam data

```{r}
spam <- read.table(file.path(PATH, 'data/spam.data'), quote="\"", comment.char="")
spam.names <- c(read.table("data/spambase.names", sep = ":", skip = 33, nrows = 53, as.is = TRUE)[,
1], "char_freq_#", read.table("data/spambase.names", sep = ":", skip = 87, nrows = 3,
as.is = TRUE)[, 1], "spam.01")
names(spam) <- spam.names
spam <- spam[sample(nrow(spam)),]
X = spam[,-58]
y = spam[,58]
```

Let us list the variables selected by the top model (that achieving the best EBIC). We use the approximation method 'ALA' that greatly speeds up computation for logistic regression. 

```{r}
pc= bicprior()  
pm= modelbbprior()  # Beta-Bin(1,1) on the model space

ms= modelSelection(y, X, priorCoef = pc, priorDelta = pm, family = 'binomial', method='ALA')
```

Let us list the variables selected by the top model (that achieving the best EBIC).
```{r}
sel.ebic= (ms[['postMode']] == 1)
which(sel.ebic)
```
## Convergence checks

We plot the trace of the number of variables at each MCMC iteration.

```{r}
nvars= rowSums(fit.bayesreg$postSample)
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(nvars, type='l', xlab='Gibbs iteration', ylab='Model size')
```

Trace plot for $\log [p(y \mid \gamma) p(\gamma)]$ which is equal to log-posterior model probabilities $\log p(\gamma \mid y)$ up to a normalizing constant. Upon convergence these probabilities should stabilize.

```{r}
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(fit.bayesreg$postProb, type='l', xlab='Gibbs iteration', ylab='log p(y | gamma) + log p(gamma)')
```

Beyond pure convergence, it is useful to assess whether the estimates of quantities of interest have stabilized. A useful quantity to look at in BMS are the marginal posterior inclusion probabilities $P(\gamma_j =1 \mid y)$, which can be estimated by the proportion of MCMC samples where $\gamma_j=1$. At iteration $b$, the estimate is

$$
\hat{P}(\gamma_j=1 \mid y)= \frac{1}{b} \sum_{j=1}^b \gamma_j^{(b)}.
$$

We obtain this estimate as it is updated after each MCMC iteration (R function cumsum obtains cumulative sums)

```{r}
margppest= matrix(NA,nrow=nrow(fit.bayesreg$postSample),ncol=ncol(fit.bayesreg$postSample))
for (j in 1:ncol(fit.bayesreg$postSample)) {
    margppest[,j]= cumsum(fit.bayesreg$postSample[,j])/(1:nrow(fit.bayesreg$postSample))
}
```

Set colours so that larger coefficients are in red, the medium ones in blue.

```{r}
col= rep('black',length(beta))
col[beta %in% c(-2/3, 1/3, 2/3)]= 'blue'
col[beta==1]= 'red'
```

```{r}
par(mar=c(4,5,.1,.1), cex.lab=1, cex.axis=1)
plot(margppest[,1], type='l', ylim=c(0,1), col=col[1], xlab='Gibbs iteration', ylab='Estimated P(gamma_j=1 | y)')
for (j in 2:ncol(margppest)) lines(margppest[,j], col=col[j])
```

The estimates at the last MCMC iteration were stored at the object `b` that we created earlier.

```{r}
par(mar=c(4,5,1,1), cex.lab=1, cex.axis=1)
plot(coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),'margpp'], col=col, xlab='Variable index', ylab='Posterior marginal inclusion\nprobability P(gamma_j=1 | y)')
```


# Homework

In Seminar 1, we used LASSO to select and predict variables on the Vessel data. Bayesian Model Selection (BMS) that we implemented in this Seminar 2 is typically more conservative than LASSO in terms of model selection, it results in more coefficients very close to 0. As a result, Bayesian Model Averaging (BMA) that relies on BMS weights may predict worse than LASSO. On the other hand LASSO is often worse at detecting the variables that truly matter (i.e. for explanatory purposes) than BMS. In this homework you will apply Bayesian Model Selection and Bayesian Model Averaging to the Vessel dataset and compare your results to those obtained with LASSO in Seminar 1. Recall that for this dataset our objective is to predict the content of compound 1 (sodium oxide) from the 1920 frequencies.

1. Load the Vessel data
2. Conduct a prior eliciation to choose $g$
3. Run a Bayesian model selection
4. Obtain predictions by Bayesian Model Averaging
4. Compare to the results obtained with LASSO


